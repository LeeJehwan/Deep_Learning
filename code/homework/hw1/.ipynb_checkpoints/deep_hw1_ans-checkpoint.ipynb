{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient descent 구현하기\n",
    "\n",
    "\n",
    "단순한 gradient descent함수를 numpy와 파이썬을 사용하여 구현해 보자.\n",
    "\n",
    "아래 함수는 W, X, Y를 인자로 받아 주어진 step 만큼 학습하는 함수이다.\n",
    "각 step수 마다 W를 개선하며 W와 cost를 출력해 보는 함수이다.\n",
    "\n",
    "##### hint: 아래 식은 W 개선 시 사용되는 식이다. (bias는 고려하지 않는다)\n",
    "<img src=\"./images/1.png\" width=60%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grad(W, X, Y, learning_rate = 0.1, step = 101):\n",
    "    for step in range(step):\n",
    "        hypothesis = np.matmul(X, W)\n",
    "        gradient = np.mean((hypothesis - Y)*X)\n",
    "        W = W - learning_rate * gradient\n",
    "        cost = np.mean(np.square(hypothesis - Y))\n",
    "        if step % 10 == 0:\n",
    "            print(step, W, cost)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 함수를 만들었다면 만든 함수를 가지고 간단한 예제를 통해 W를 학습 시켜보자.\n",
    "\n",
    "정상적으로 만들었다면 W는 2에 가까워야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[1.20142826]] 7.169363580736932\n",
      "10 [[1.86056541]] 0.014880838197587232\n",
      "20 [[1.86179274]] 0.014856032422641835\n",
      "30 [[1.86179503]] 0.014856032336636136\n",
      "40 [[1.86179503]] 0.01485603233663581\n",
      "50 [[1.86179503]] 0.014856032336635754\n",
      "60 [[1.86179503]] 0.01485603233663581\n",
      "70 [[1.86179503]] 0.01485603233663581\n",
      "80 [[1.86179503]] 0.01485603233663581\n",
      "90 [[1.86179503]] 0.01485603233663581\n",
      "100 [[1.86179503]] 0.01485603233663581\n",
      "W: [[1.86179503]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3]])  # (3,1)\n",
    "Y = np.array([[2], [4], [6]])  # (3,1)\n",
    "W = np.random.random((1,1))    # (1,1)\n",
    "\n",
    "W = grad(W, X, Y)\n",
    "print(\"W:\",W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "### 논리회로 만들어보기\n",
    "\n",
    "우리는 기존에 과제로 AND게이트를 만들어 본 적이 있다. 당시 W의 값을 직접 구했었는데 이번엔 위에서 우리가 만든 gradient descent로 인공지능에게 구하라고 시켜보자\n",
    "\n",
    "\n",
    "* 다음은 AND게이트의 진리표이다.\n",
    "\n",
    "|x1|x2| y|\n",
    "|-|-|-|\n",
    "|0|0|0|\n",
    "|1|0|0|\n",
    "|0|1|0|\n",
    "|1|1|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[0.37161906]\n",
      " [0.39344269]] 0.8050612183717711\n",
      "10 [[0.15456724]\n",
      " [0.17639087]] 0.582559584275634\n",
      "20 [[0.0550311 ]\n",
      " [0.07685473]] 0.5357679881685533\n",
      "30 [[0.00938559]\n",
      " [0.03120922]] 0.5259278201394855\n",
      "40 [[-0.01154664]\n",
      " [ 0.01027699]] 0.5238584548069009\n",
      "50 [[-0.02114579]\n",
      " [ 0.00067784]] 0.5234232719048353\n",
      "60 [[-0.02554779]\n",
      " [-0.00372416]] 0.5233317539136579\n",
      "70 [[-0.02756647]\n",
      " [-0.00574284]] 0.5233125078847598\n",
      "80 [[-0.0284922 ]\n",
      " [-0.00666857]] 0.5233084604879191\n",
      "90 [[-0.02891672]\n",
      " [-0.00709309]] 0.5233076093294141\n",
      "100 [[-0.0291114 ]\n",
      " [-0.00728777]] 0.5233074303326838\n"
     ]
    }
   ],
   "source": [
    "# insert your code\n",
    "# 행렬의 size에 유의하자\n",
    "# 다른 게이트와 구분하기 위해 W의 이름을 W_and 라 하자.\n",
    "\n",
    "x = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "y = np.array([[0],[0],[0],[1]])\n",
    "W_and = np.random.random((2,1))\n",
    "\n",
    "W_and = grad(W_and, x, y, learning_rate = 0.1, step = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 끝났다면 학습한 W값을 가지고 AND게이트가 학습이 제대로 이뤄졌는지 확인해보자.\n",
    "만일 학습이 제대로 이루어지지 않았다면 learning_rate나 step을 변경시켜 튜닝을 해주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    y = np.matmul(x, W_and)\n",
    "    if y >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(AND(0,0))\n",
    "print(AND(1,0))\n",
    "print(AND(0,1))\n",
    "print(AND(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마찬가지로 OR게이트와 NAND게이트도 만들어 보자.\n",
    "\n",
    "* 다음은 OR게이트의 진리표이다.\n",
    "\n",
    "|x1|x2| y|\n",
    "|-|-|-|\n",
    "|0|0|0|\n",
    "|1|0|1|\n",
    "|0|1|1|\n",
    "|1|1|1|\n",
    "\n",
    "* 다음은 NAND게이트의 진리표이다.\n",
    "\n",
    "|x1|x2| y|\n",
    "|-|-|-|\n",
    "|0|0|1|\n",
    "|1|0|1|\n",
    "|0|1|1|\n",
    "|1|1|0|\n",
    "\n",
    "여기서도 W_or로 가중치의 이름을 정해 주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[0.36828894]\n",
      " [0.34817503]] 0.2501601543388304\n",
      "10 [[0.53528093]\n",
      " [0.51516701]] 0.11845657693913147\n",
      "20 [[0.6118605 ]\n",
      " [0.59174658]] 0.09075960911874394\n",
      "30 [[0.64697854]\n",
      " [0.62686463]] 0.08493499872023141\n",
      "40 [[0.66308306]\n",
      " [0.64296914]] 0.08371009617803012\n",
      "50 [[0.6704683 ]\n",
      " [0.65035438]] 0.08345250191245068\n",
      "60 [[0.67385504]\n",
      " [0.65374113]] 0.08339833041441447\n",
      "70 [[0.67540814]\n",
      " [0.65529423]] 0.08338693826951216\n",
      "80 [[0.67612037]\n",
      " [0.65600645]] 0.08338454252691355\n",
      "90 [[0.67644698]\n",
      " [0.65633307]] 0.08338403870760226\n",
      "100 [[0.67659676]\n",
      " [0.65648284]] 0.0833839327555277\n",
      "110 [[0.67666545]\n",
      " [0.65655153]] 0.08338391047404334\n",
      "120 [[0.67669695]\n",
      " [0.65658303]] 0.08338390578829702\n",
      "130 [[0.67671139]\n",
      " [0.65659747]] 0.08338390480289504\n",
      "140 [[0.67671801]\n",
      " [0.6566041 ]] 0.08338390459566719\n",
      "150 [[0.67672105]\n",
      " [0.65660714]] 0.08338390455208765\n",
      "160 [[0.67672245]\n",
      " [0.65660853]] 0.08338390454292297\n",
      "170 [[0.67672308]\n",
      " [0.65660917]] 0.08338390454099565\n",
      "180 [[0.67672338]\n",
      " [0.65660946]] 0.08338390454059036\n",
      "190 [[0.67672351]\n",
      " [0.65660959]] 0.08338390454050512\n"
     ]
    }
   ],
   "source": [
    "# insert your code\n",
    "# 다른 게이트와 구분하기 위해 W의 이름을 W_or 라 하자.\n",
    "\n",
    "x = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "y = np.array([[0],[1],[1],[1]])\n",
    "W_or = np.random.random((2,1))\n",
    "\n",
    "W_or = grad(W_and, x, y, step = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    y = np.matmul(x, W_or)\n",
    "    if y >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(OR(0,0))\n",
    "print(OR(1,0))\n",
    "print(OR(0,1))\n",
    "print(OR(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAND게이트의 경우 AND게이트를 반전시켜주면 되므로 별도로 학습을 하지 않아도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make nand function\n",
    "\n",
    "def NAND(x1, x2):\n",
    "    return int(not AND(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(NAND(0,0))\n",
    "print(NAND(1,0))\n",
    "print(NAND(0,1))\n",
    "print(NAND(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 XOR게이트를 만들어보자\n",
    "기존과 같은 방식으로 학습시켜보자.\n",
    "\n",
    "* 다음은 XOR게이트의 진리표이다.\n",
    "\n",
    "|x1|x2| y|\n",
    "|-|-|-|\n",
    "|0|0|0|\n",
    "|1|0|1|\n",
    "|0|1|1|\n",
    "|1|1|0|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[0.99264397]\n",
      " [0.96563904]] 0.9590361654843087\n",
      "10 [[0.99259553]\n",
      " [0.96559061]] 0.9589423304191458\n",
      "20 [[0.9925471 ]\n",
      " [0.96554218]] 0.9588485094282401\n",
      "30 [[0.99249867]\n",
      " [0.96549375]] 0.9587547025094798\n",
      "40 [[0.99245025]\n",
      " [0.96544533]] 0.9586609096607553\n",
      "50 [[0.99240183]\n",
      " [0.96539691]] 0.9585671308799555\n",
      "60 [[0.99235342]\n",
      " [0.96534849]] 0.9584733661649708\n",
      "70 [[0.992305  ]\n",
      " [0.96530008]] 0.9583796155136907\n",
      "80 [[0.9922566 ]\n",
      " [0.96525167]] 0.9582858789240065\n",
      "90 [[0.99220819]\n",
      " [0.96520327]] 0.9581921563938088\n",
      "100 [[0.99215979]\n",
      " [0.96515487]] 0.9580984479209899\n",
      "110 [[0.99211139]\n",
      " [0.96510647]] 0.9580047535034397\n",
      "120 [[0.992063  ]\n",
      " [0.96505807]] 0.9579110731390513\n",
      "130 [[0.99201461]\n",
      " [0.96500968]] 0.9578174068257163\n",
      "140 [[0.99196622]\n",
      " [0.9649613 ]] 0.9577237545613272\n",
      "150 [[0.99191784]\n",
      " [0.96491291]] 0.9576301163437768\n",
      "160 [[0.99186946]\n",
      " [0.96486453]] 0.9575364921709582\n",
      "170 [[0.99182108]\n",
      " [0.96481616]] 0.9574428820407652\n",
      "180 [[0.99177271]\n",
      " [0.96476779]] 0.9573492859510916\n",
      "190 [[0.99172434]\n",
      " [0.96471942]] 0.9572557038998306\n",
      "200 [[0.99167598]\n",
      " [0.96467105]] 0.9571621358848773\n",
      "210 [[0.99162761]\n",
      " [0.96462269]] 0.957068581904126\n",
      "220 [[0.99157926]\n",
      " [0.96457433]] 0.9569750419554723\n",
      "230 [[0.9915309 ]\n",
      " [0.96452598]] 0.9568815160368112\n",
      "240 [[0.99148255]\n",
      " [0.96447763]] 0.9567880041460386\n",
      "250 [[0.99143421]\n",
      " [0.96442928]] 0.9566945062810499\n",
      "260 [[0.99138586]\n",
      " [0.96438094]] 0.956601022439742\n",
      "270 [[0.99133752]\n",
      " [0.9643326 ]] 0.9565075526200112\n",
      "280 [[0.99128919]\n",
      " [0.96428426]] 0.9564140968197541\n",
      "290 [[0.99124085]\n",
      " [0.96423593]] 0.9563206550368687\n",
      "300 [[0.99119253]\n",
      " [0.9641876 ]] 0.9562272272692521\n",
      "310 [[0.9911442 ]\n",
      " [0.96413928]] 0.9561338135148024\n",
      "320 [[0.99109588]\n",
      " [0.96409096]] 0.9560404137714172\n",
      "330 [[0.99104756]\n",
      " [0.96404264]] 0.9559470280369958\n",
      "340 [[0.99099925]\n",
      " [0.96399432]] 0.955853656309436\n",
      "350 [[0.99095094]\n",
      " [0.96394601]] 0.9557602985866379\n",
      "360 [[0.99090263]\n",
      " [0.96389771]] 0.9556669548665003\n",
      "370 [[0.99085433]\n",
      " [0.9638494 ]] 0.9555736251469229\n",
      "380 [[0.99080603]\n",
      " [0.9638011 ]] 0.9554803094258066\n",
      "390 [[0.99075773]\n",
      " [0.96375281]] 0.9553870077010508\n",
      "400 [[0.99070944]\n",
      " [0.96370451]] 0.9552937199705569\n",
      "410 [[0.99066115]\n",
      " [0.96365622]] 0.9552004462322254\n",
      "420 [[0.99061286]\n",
      " [0.96360794]] 0.9551071864839576\n",
      "430 [[0.99056458]\n",
      " [0.96355966]] 0.9550139407236558\n",
      "440 [[0.9905163 ]\n",
      " [0.96351138]] 0.9549207089492212\n",
      "450 [[0.99046803]\n",
      " [0.96346311]] 0.9548274911585563\n",
      "460 [[0.99041976]\n",
      " [0.96341483]] 0.9547342873495642\n",
      "470 [[0.99037149]\n",
      " [0.96336657]] 0.9546410975201475\n",
      "480 [[0.99032323]\n",
      " [0.9633183 ]] 0.954547921668209\n",
      "490 [[0.99027497]\n",
      " [0.96327004]] 0.9544547597916526\n",
      "500 [[0.99022671]\n",
      " [0.96322179]] 0.9543616118883818\n",
      "510 [[0.99017846]\n",
      " [0.96317353]] 0.9542684779563008\n",
      "520 [[0.99013021]\n",
      " [0.96312529]] 0.9541753579933147\n",
      "530 [[0.99008196]\n",
      " [0.96307704]] 0.9540822519973278\n",
      "540 [[0.99003372]\n",
      " [0.9630288 ]] 0.953989159966245\n",
      "550 [[0.98998548]\n",
      " [0.96298056]] 0.9538960818979721\n",
      "560 [[0.98993725]\n",
      " [0.96293233]] 0.9538030177904151\n",
      "570 [[0.98988902]\n",
      " [0.96288409]] 0.9537099676414796\n",
      "580 [[0.98984079]\n",
      " [0.96283587]] 0.9536169314490722\n",
      "590 [[0.98979257]\n",
      " [0.96278764]] 0.9535239092110994\n",
      "600 [[0.98974435]\n",
      " [0.96273942]] 0.9534309009254681\n",
      "610 [[0.98969613]\n",
      " [0.96269121]] 0.9533379065900855\n",
      "620 [[0.98964792]\n",
      " [0.96264299]] 0.9532449262028594\n",
      "630 [[0.98959971]\n",
      " [0.96259478]] 0.9531519597616974\n",
      "640 [[0.9895515 ]\n",
      " [0.96254658]] 0.9530590072645085\n",
      "650 [[0.9895033 ]\n",
      " [0.96249838]] 0.9529660687092006\n",
      "660 [[0.9894551 ]\n",
      " [0.96245018]] 0.9528731440936827\n",
      "670 [[0.98940691]\n",
      " [0.96240198]] 0.9527802334158642\n",
      "680 [[0.98935872]\n",
      " [0.96235379]] 0.9526873366736544\n",
      "690 [[0.98931053]\n",
      " [0.9623056 ]] 0.9525944538649632\n",
      "700 [[0.98926234]\n",
      " [0.96225742]] 0.9525015849877007\n",
      "710 [[0.98921416]\n",
      " [0.96220924]] 0.9524087300397766\n",
      "720 [[0.98916599]\n",
      " [0.96216106]] 0.9523158890191031\n",
      "730 [[0.98911781]\n",
      " [0.96211289]] 0.9522230619235903\n",
      "740 [[0.98906964]\n",
      " [0.96206472]] 0.9521302487511497\n",
      "750 [[0.98902148]\n",
      " [0.96201656]] 0.9520374494996932\n",
      "760 [[0.98897332]\n",
      " [0.96196839]] 0.9519446641671326\n",
      "770 [[0.98892516]\n",
      " [0.96192023]] 0.9518518927513799\n",
      "780 [[0.988877  ]\n",
      " [0.96187208]] 0.9517591352503488\n",
      "790 [[0.98882885]\n",
      " [0.96182393]] 0.951666391661951\n",
      "800 [[0.9887807 ]\n",
      " [0.96177578]] 0.9515736619841005\n",
      "810 [[0.98873256]\n",
      " [0.96172764]] 0.9514809462147107\n",
      "820 [[0.98868442]\n",
      " [0.9616795 ]] 0.951388244351695\n",
      "830 [[0.98863628]\n",
      " [0.96163136]] 0.9512955563929683\n",
      "840 [[0.98858815]\n",
      " [0.96158322]] 0.9512028823364448\n",
      "850 [[0.98854002]\n",
      " [0.96153509]] 0.9511102221800395\n",
      "860 [[0.98849189]\n",
      " [0.96148697]] 0.9510175759216677\n",
      "870 [[0.98844377]\n",
      " [0.96143885]] 0.9509249435592443\n",
      "880 [[0.98839565]\n",
      " [0.96139073]] 0.9508323250906854\n",
      "890 [[0.98834754]\n",
      " [0.96134261]] 0.9507397205139065\n",
      "900 [[0.98829942]\n",
      " [0.9612945 ]] 0.9506471298268246\n",
      "910 [[0.98825132]\n",
      " [0.96124639]] 0.9505545530273564\n",
      "920 [[0.98820321]\n",
      " [0.96119829]] 0.9504619901134188\n",
      "930 [[0.98815511]\n",
      " [0.96115019]] 0.9503694410829293\n",
      "940 [[0.98810701]\n",
      " [0.96110209]] 0.9502769059338051\n",
      "950 [[0.98805892]\n",
      " [0.961054  ]] 0.9501843846639645\n",
      "960 [[0.98801083]\n",
      " [0.96100591]] 0.9500918772713249\n",
      "970 [[0.98796274]\n",
      " [0.96095782]] 0.9499993837538064\n",
      "980 [[0.98791466]\n",
      " [0.96090974]] 0.9499069041093265\n",
      "990 [[0.98786658]\n",
      " [0.96086166]] 0.9498144383358053\n",
      "1000 [[0.9878185 ]\n",
      " [0.96081358]] 0.9497219864311613\n",
      "1010 [[0.98777043]\n",
      " [0.96076551]] 0.9496295483933157\n",
      "1020 [[0.98772236]\n",
      " [0.96071744]] 0.9495371242201879\n",
      "1030 [[0.9876743 ]\n",
      " [0.96066938]] 0.9494447139096984\n",
      "1040 [[0.98762624]\n",
      " [0.96062131]] 0.9493523174597676\n",
      "1050 [[0.98757818]\n",
      " [0.96057326]] 0.9492599348683168\n",
      "1060 [[0.98753013]\n",
      " [0.9605252 ]] 0.9491675661332674\n",
      "1070 [[0.98748208]\n",
      " [0.96047715]] 0.9490752112525411\n",
      "1080 [[0.98743403]\n",
      " [0.96042911]] 0.9489828702240598\n",
      "1090 [[0.98738599]\n",
      " [0.96038106]] 0.9488905430457459\n",
      "1100 [[0.98733795]\n",
      " [0.96033302]] 0.948798229715522\n",
      "1110 [[0.98728991]\n",
      " [0.96028499]] 0.9487059302313112\n",
      "1120 [[0.98724188]\n",
      " [0.96023695]] 0.9486136445910368\n",
      "1130 [[0.98719385]\n",
      " [0.96018892]] 0.948521372792622\n",
      "1140 [[0.98714582]\n",
      " [0.9601409 ]] 0.9484291148339908\n",
      "1150 [[0.9870978 ]\n",
      " [0.96009288]] 0.9483368707130676\n",
      "1160 [[0.98704978]\n",
      " [0.96004486]] 0.9482446404277768\n",
      "1170 [[0.98700177]\n",
      " [0.95999685]] 0.9481524239760434\n",
      "1180 [[0.98695376]\n",
      " [0.95994883]] 0.9480602213557917\n",
      "1190 [[0.98690575]\n",
      " [0.95990083]] 0.947968032564948\n",
      "1200 [[0.98685775]\n",
      " [0.95985282]] 0.9478758576014373\n",
      "1210 [[0.98680975]\n",
      " [0.95980482]] 0.9477836964631866\n",
      "1220 [[0.98676175]\n",
      " [0.95975683]] 0.9476915491481211\n",
      "1230 [[0.98671376]\n",
      " [0.95970883]] 0.9475994156541686\n",
      "1240 [[0.98666577]\n",
      " [0.95966085]] 0.9475072959792554\n",
      "1250 [[0.98661778]\n",
      " [0.95961286]] 0.9474151901213088\n",
      "1260 [[0.9865698 ]\n",
      " [0.95956488]] 0.9473230980782564\n",
      "1270 [[0.98652182]\n",
      " [0.9595169 ]] 0.9472310198480266\n",
      "1280 [[0.98647385]\n",
      " [0.95946892]] 0.9471389554285473\n",
      "1290 [[0.98642588]\n",
      " [0.95942095]] 0.9470469048177469\n",
      "1300 [[0.98637791]\n",
      " [0.95937299]] 0.9469548680135544\n",
      "1310 [[0.98632995]\n",
      " [0.95932502]] 0.946862845013899\n",
      "1320 [[0.98628198]\n",
      " [0.95927706]] 0.9467708358167103\n",
      "1330 [[0.98623403]\n",
      " [0.9592291 ]] 0.9466788404199179\n",
      "1340 [[0.98618607]\n",
      " [0.95918115]] 0.9465868588214519\n",
      "1350 [[0.98613813]\n",
      " [0.9591332 ]] 0.9464948910192426\n",
      "1360 [[0.98609018]\n",
      " [0.95908526]] 0.9464029370112208\n",
      "1370 [[0.98604224]\n",
      " [0.95903731]] 0.9463109967953172\n",
      "1380 [[0.9859943 ]\n",
      " [0.95898937]] 0.9462190703694634\n",
      "1390 [[0.98594636]\n",
      " [0.95894144]] 0.9461271577315913\n",
      "1400 [[0.98589843]\n",
      " [0.95889351]] 0.946035258879632\n",
      "1410 [[0.9858505 ]\n",
      " [0.95884558]] 0.9459433738115186\n",
      "1420 [[0.98580258]\n",
      " [0.95879765]] 0.9458515025251838\n",
      "1430 [[0.98575466]\n",
      " [0.95874973]] 0.9457596450185599\n",
      "1440 [[0.98570674]\n",
      " [0.95870182]] 0.9456678012895803\n",
      "1450 [[0.98565883]\n",
      " [0.9586539 ]] 0.9455759713361783\n",
      "1460 [[0.98561092]\n",
      " [0.95860599]] 0.9454841551562879\n",
      "1470 [[0.98556301]\n",
      " [0.95855809]] 0.9453923527478434\n",
      "1480 [[0.98551511]\n",
      " [0.95851018]] 0.945300564108779\n",
      "1490 [[0.98546721]\n",
      " [0.95846228]] 0.9452087892370299\n",
      "1500 [[0.98541931]\n",
      " [0.95841439]] 0.9451170281305306\n",
      "1510 [[0.98537142]\n",
      " [0.9583665 ]] 0.9450252807872165\n",
      "1520 [[0.98532353]\n",
      " [0.95831861]] 0.9449335472050238\n",
      "1530 [[0.98527565]\n",
      " [0.95827072]] 0.9448418273818877\n",
      "1540 [[0.98522776]\n",
      " [0.95822284]] 0.9447501213157447\n",
      "1550 [[0.98517989]\n",
      " [0.95817496]] 0.9446584290045317\n",
      "1560 [[0.98513201]\n",
      " [0.95812709]] 0.9445667504461854\n",
      "1570 [[0.98508414]\n",
      " [0.95807922]] 0.9444750856386434\n",
      "1580 [[0.98503628]\n",
      " [0.95803135]] 0.944383434579843\n",
      "1590 [[0.98498841]\n",
      " [0.95798349]] 0.9442917972677219\n",
      "1600 [[0.98494055]\n",
      " [0.95793563]] 0.9442001737002182\n",
      "1610 [[0.9848927 ]\n",
      " [0.95788777]] 0.9441085638752702\n",
      "1620 [[0.98484484]\n",
      " [0.95783992]] 0.9440169677908176\n",
      "1630 [[0.98479699]\n",
      " [0.95779207]] 0.9439253854447982\n",
      "1640 [[0.98474915]\n",
      " [0.95774422]] 0.9438338168351517\n",
      "1650 [[0.98470131]\n",
      " [0.95769638]] 0.9437422619598186\n",
      "1660 [[0.98465347]\n",
      " [0.95764854]] 0.9436507208167381\n",
      "1670 [[0.98460563]\n",
      " [0.95760071]] 0.9435591934038505\n",
      "1680 [[0.9845578 ]\n",
      " [0.95755288]] 0.9434676797190973\n",
      "1690 [[0.98450997]\n",
      " [0.95750505]] 0.9433761797604187\n",
      "1700 [[0.98446215]\n",
      " [0.95745723]] 0.9432846935257556\n",
      "1710 [[0.98441433]\n",
      " [0.95740941]] 0.9431932210130504\n",
      "1720 [[0.98436651]\n",
      " [0.95736159]] 0.9431017622202442\n",
      "1730 [[0.9843187 ]\n",
      " [0.95731378]] 0.9430103171452798\n",
      "1740 [[0.98427089]\n",
      " [0.95726597]] 0.9429188857860995\n",
      "1750 [[0.98422308]\n",
      " [0.95721816]] 0.9428274681406451\n",
      "1760 [[0.98417528]\n",
      " [0.95717036]] 0.9427360642068608\n",
      "1770 [[0.98412748]\n",
      " [0.95712256]] 0.9426446739826899\n",
      "1780 [[0.98407969]\n",
      " [0.95707476]] 0.9425532974660759\n",
      "1790 [[0.9840319 ]\n",
      " [0.95702697]] 0.9424619346549625\n",
      "1800 [[0.98398411]\n",
      " [0.95697918]] 0.9423705855472949\n",
      "1810 [[0.98393632]\n",
      " [0.9569314 ]] 0.9422792501410171\n",
      "1820 [[0.98388854]\n",
      " [0.95688362]] 0.9421879284340743\n",
      "1830 [[0.98384077]\n",
      " [0.95683584]] 0.9420966204244113\n",
      "1840 [[0.98379299]\n",
      " [0.95678807]] 0.942005326109974\n",
      "1850 [[0.98374522]\n",
      " [0.9567403 ]] 0.9419140454887079\n",
      "1860 [[0.98369746]\n",
      " [0.95669253]] 0.9418227785585598\n",
      "1870 [[0.98364969]\n",
      " [0.95664477]] 0.9417315253174757\n",
      "1880 [[0.98360193]\n",
      " [0.95659701]] 0.9416402857634028\n",
      "1890 [[0.98355418]\n",
      " [0.95654925]] 0.9415490598942882\n",
      "1900 [[0.98350642]\n",
      " [0.9565015 ]] 0.9414578477080788\n",
      "1910 [[0.98345868]\n",
      " [0.95645375]] 0.9413666492027225\n",
      "1920 [[0.98341093]\n",
      " [0.95640601]] 0.941275464376167\n",
      "1930 [[0.98336319]\n",
      " [0.95635827]] 0.941184293226361\n",
      "1940 [[0.98331545]\n",
      " [0.95631053]] 0.9410931357512533\n",
      "1950 [[0.98326772]\n",
      " [0.95626279]] 0.9410019919487929\n",
      "1960 [[0.98321999]\n",
      " [0.95621506]] 0.940910861816929\n",
      "1970 [[0.98317226]\n",
      " [0.95616734]] 0.9408197453536107\n",
      "1980 [[0.98312454]\n",
      " [0.95611961]] 0.9407286425567883\n",
      "1990 [[0.98307682]\n",
      " [0.95607189]] 0.9406375534244117\n"
     ]
    }
   ],
   "source": [
    "# insert your code\n",
    "# 다른 게이트와 구분하기 위해 W의 이름을 W_xor 라 하자.\n",
    "\n",
    "x = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "W_xor = np.random.random((2,1))\n",
    "\n",
    "W_xor = grad(W_xor, x, y, learning_rate = 1e-5, step = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    y = np.matmul(x, W_xor)\n",
    "    if y >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(XOR(0,0))\n",
    "print(XOR(1,0))\n",
    "print(XOR(0,1))\n",
    "print(XOR(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아마 아무리 학습을 하여도 제대로 된 결과가 나오지 않을 것 이다. \n",
    "이는 단순 직선으로는 XOR게이트를 만들 수 없기 떄문이다.\n",
    "\n",
    "<img src=\"./images/2.png\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 비선형 구조는 선형 구조의 분류자를 층을 쌓아서 조합해 비선형 구조로 만들어야한다.\n",
    "\n",
    "따라서 우리는 우리가 만든 AND, OR, NAND게이트를 조합하여 XOR를 만들어야 한다.\n",
    "\n",
    "\n",
    "\n",
    "##### hint: 조합 방법은 다음과 같다.\n",
    "\n",
    "<img src=\"./images/3.png\" width=60%/>\n",
    "\n",
    "\n",
    "위 방법은 마치 신경망 구조와 같다. 따라서 이를 다시 신경망 처럼 표현하면 다음과 같다.\n",
    "\n",
    "<img src=\"./images/4.png\" width=80%/>\n",
    "\n",
    "위 구조를 이용해서 XOR게이트 함수를 직접 구현해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make XOR gate\n",
    "\n",
    "def XOR(x1, x2):\n",
    "    s1 = NAND(x1,x2)\n",
    "    s2 = OR(x1,x2)\n",
    "    return AND(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(XOR(0,0))\n",
    "print(XOR(1,0))\n",
    "print(XOR(0,1))\n",
    "print(XOR(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example1\n",
    "\n",
    "### tensorflow 기초 실습\n",
    "\n",
    "앞에서 XOR게이트를 만들 때 본 신경망 구조는 tensorflow의 작동방식과 일치한다.\n",
    "\n",
    "tensorflow를 익히기 위해 간단한 예제를 실습해 보자.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./images/5.png\" width=70%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define a computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.placeholder(tf.int32, name=\"input_a\")\n",
    "b = tf.placeholder(tf.int32, name=\"input_b\")\n",
    "\n",
    "c = tf.add(a, b, name=\"add\")\n",
    "d = tf.multiply(a, b, name=\"multiply\")\n",
    "e = tf.subtract(c, d, name=\"subtract\")\n",
    "out = tf.add(b, e, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 tensorflow를 이용하여 operation(Node)와 tensor(edge)를 정의하여 그래프를 정의해준다. \n",
    "\n",
    "이는 단순히 그래프를 정의한 것으로 Session 객체로 run 하기 전까지는 실행이 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: -8\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "input_data = { a: 7, b: 3 }\n",
    "result = sess.run(out, feed_dict=input_data)\n",
    "print(\"out:\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "### tensorflow를 이용하여 계산해보기\n",
    "\n",
    "다음 그래프를 정의하고 계산해 보자.\n",
    "<img src=\"./images/6.png\" width=70%/>\n",
    "\n",
    "\n",
    "tensorflow의 수학 관련 Operations 에 관한 설명은 [이곳을 참고하자](https://www.tensorflow.org/api_guides/python/math_ops)\n",
    "\n",
    "그래프 실행 후 결과는 다음과 같다\n",
    "\n",
    "|In | Out|\n",
    "|---|----|\n",
    "|1, 2, 3| 15|\n",
    "|-1, -2, 3| -3|\n",
    "|123, 456, 789|44613795|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "in1 = tf.placeholder(tf.int32, name=\"input_1\")\n",
    "in2 = tf.placeholder(tf.int32, name=\"input_2\")\n",
    "in3 = tf.placeholder(tf.int32, name=\"input_3\")\n",
    "\n",
    "prod_list = [in1, in2, in3]\n",
    "a = tf.add(in1, in2, name = \"add\")\n",
    "b = tf.reduce_prod(prod_list, name = \"product\")\n",
    "c = tf.multiply(in2, in3, name = \"mul\")\n",
    "\n",
    "sum_list = [a, b, c]\n",
    "out = tf.reduce_sum(sum_list, name='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: 15\n",
      "out: -3\n",
      "out: 44613795\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "input_data1 = { in1: 1, in2: 2, in3: 3}\n",
    "input_data2 = { in1: -1, in2: -2, in3: 3}\n",
    "input_data3 = { in1: 123, in2: 456, in3: 789}\n",
    "\n",
    "\n",
    "result1 = sess.run(out, feed_dict=input_data1)\n",
    "result2 = sess.run(out, feed_dict=input_data2)\n",
    "result3 = sess.run(out, feed_dict=input_data3)\n",
    "print(\"out:\",result1)\n",
    "print(\"out:\",result2)\n",
    "print(\"out:\",result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
