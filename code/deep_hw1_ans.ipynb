{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient descent 구현하기\n",
    "\n",
    "\n",
    "단순한 gradient descent함수를 numpy와 파이썬을 사용하여 구현해 보자.\n",
    "\n",
    "아래 함수는 W, X, Y를 인자로 받아 주어진 step 만큼 학습하는 함수이다.\n",
    "각 step수 마다 W를 개선하며 W와 cost를 출력해 보는 함수이다.\n",
    "\n",
    "##### hint: 아래 식은 W 개선 시 사용되는 식이다. (bias는 고려하지 않는다)\n",
    "<img src=\"./images/1.png\" width=60%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grad(W, X, Y, learning_rate = 0.1, step = 101):\n",
    "    for step in range(step):\n",
    "        hypothesis = np.matmul(X, W)\n",
    "        gradient = np.mean((hypothesis - Y)*X)\n",
    "        W = W - learning_rate * gradient\n",
    "        cost = np.mean(np.square(hypothesis - Y))\n",
    "        if step % 10 == 0:\n",
    "            print(step, W, cost)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 함수를 만들었다면 만든 함수를 가지고 간단한 예제를 통해 W를 학습 시켜보자.\n",
    "\n",
    "정상적으로 만들었다면 W는 2에 가까워야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[1.34208224]] 7.101540196756001\n",
      "10 [[1.99877494]] 2.4622214383048002e-05\n",
      "20 [[1.99999772]] 8.536928952783315e-11\n",
      "30 [[2.]] 2.9598945147842967e-16\n",
      "40 [[2.]] 1.0262748164589731e-21\n",
      "50 [[2.]] 3.5377782062811645e-27\n",
      "60 [[2.]] 0.0\n",
      "70 [[2.]] 0.0\n",
      "80 [[2.]] 0.0\n",
      "90 [[2.]] 0.0\n",
      "100 [[2.]] 0.0\n",
      "W: [[2.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3]])  # (3,1)\n",
    "Y = np.array([[2], [4], [6]])  # (3,1)\n",
    "W = np.random.random((1,1))    # (1,1)\n",
    "\n",
    "W = grad(W, X, Y)\n",
    "print(\"W:\",W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "### 논리회로 만들어보기\n",
    "\n",
    "우리는 기존에 과제로 AND게이트를 만들어 본 적이 있다. 당시 W의 값을 직접 구했었는데 이번엔 위에서 우리가 만든 gradient descent로 인공지능에게 구하라고 시켜보자\n",
    "\n",
    "\n",
    "* 다음은 AND게이트의 진리표이다.\n",
    "\n",
    "|x1|x2| y|\n",
    "|-|-|-|\n",
    "|0|0|0|\n",
    "|1|0|0|\n",
    "|0|1|0|\n",
    "|1|1|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[0.05707019]\n",
      " [0.13894981]] 0.1812531567061\n",
      "10 [[0.1844784 ]\n",
      " [0.26635802]] 0.1045874505491565\n",
      "20 [[0.24290555]\n",
      " [0.32478517]] 0.08846482397909793\n",
      "30 [[0.26969921]\n",
      " [0.35157883]] 0.0850742716637585\n",
      "40 [[0.28198631]\n",
      " [0.36386593]] 0.08436124609356987\n",
      "50 [[0.28762096]\n",
      " [0.36950058]] 0.08421129841057952\n",
      "60 [[0.29020491]\n",
      " [0.37208453]] 0.08417976474814967\n",
      "70 [[0.29138986]\n",
      " [0.37326948]] 0.08417313328945414\n",
      "80 [[0.29193326]\n",
      " [0.37381288]] 0.0841717387085193\n",
      "90 [[0.29218246]\n",
      " [0.37406208]] 0.08417144543126717\n",
      "100 [[0.29229673]\n",
      " [0.37417635]] 0.08417138375571685\n",
      "110 [[0.29234914]\n",
      " [0.37422876]] 0.08417137078548656\n",
      "120 [[0.29237317]\n",
      " [0.37425279]] 0.08417136805787612\n",
      "130 [[0.29238419]\n",
      " [0.37426381]] 0.08417136748426574\n",
      "140 [[0.29238924]\n",
      " [0.37426886]] 0.08417136736363677\n",
      "150 [[0.29239156]\n",
      " [0.37427118]] 0.08417136733826874\n",
      "160 [[0.29239262]\n",
      " [0.37427224]] 0.08417136733293391\n",
      "170 [[0.29239311]\n",
      " [0.37427273]] 0.08417136733181202\n",
      "180 [[0.29239333]\n",
      " [0.37427295]] 0.0841713673315761\n",
      "190 [[0.29239344]\n",
      " [0.37427306]] 0.08417136733152644\n",
      "200 [[0.29239348]\n",
      " [0.3742731 ]] 0.08417136733151603\n"
     ]
    }
   ],
   "source": [
    "# insert your code\n",
    "# 행렬의 size에 유의하자\n",
    "# 다른 게이트와 구분하기 위해 W의 이름을 W_and 라 하자.\n",
    "\n",
    "x = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "y = np.array([[0],[0],[0],[1]])\n",
    "W_and = np.random.random((2,1))\n",
    "\n",
    "W_and = grad(W_and, x, y, step = 201)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 끝났다면 학습한 W값을 가지고 AND게이트가 학습이 제대로 이뤄졌는지 확인해보자.\n",
    "만일 학습이 제대로 이루어지지 않았다면 learning_rate나 step을 변경시켜 튜닝을 해주자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    y = np.matmul(x, W_and)\n",
    "    if y >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(AND(0,0))\n",
    "print(AND(1,0))\n",
    "print(AND(0,1))\n",
    "print(AND(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마찬가지로 OR게이트와 NAND게이트도 만들어 보자.\n",
    "\n",
    "* 다음은 OR게이트의 진리표이다.\n",
    "\n",
    "|x1|x2| y|\n",
    "|-|-|-|\n",
    "|0|0|0|\n",
    "|1|0|1|\n",
    "|0|1|1|\n",
    "|1|1|1|\n",
    "\n",
    "* 다음은 NAND게이트의 진리표이다.\n",
    "\n",
    "|x1|x2| y|\n",
    "|-|-|-|\n",
    "|0|0|1|\n",
    "|1|0|1|\n",
    "|0|1|1|\n",
    "|1|1|0|\n",
    "\n",
    "여기서도 W_or로 가중치의 이름을 정해 주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[0.31739349]\n",
      " [0.39927311]] 0.25083807380995704\n",
      "10 [[0.48433062]\n",
      " [0.56621024]] 0.11922100301493996\n",
      "20 [[0.56088504]\n",
      " [0.64276466]] 0.09154222734007668\n",
      "30 [[0.59599154]\n",
      " [0.67787116]] 0.08572144270908783\n",
      "40 [[0.61209077]\n",
      " [0.69397039]] 0.08449734471724184\n",
      "50 [[0.61947359]\n",
      " [0.70135321]] 0.08423991964680307\n",
      "60 [[0.62285922]\n",
      " [0.70473884]] 0.0841857837301266\n",
      "70 [[0.62441181]\n",
      " [0.70629143]] 0.08417439906790466\n",
      "80 [[0.6251238 ]\n",
      " [0.70700342]] 0.08417200489889701\n",
      "90 [[0.62545031]\n",
      " [0.70732992]] 0.08417150141050836\n",
      "100 [[0.62560004]\n",
      " [0.70747965]] 0.08417139552802613\n",
      "110 [[0.6256687 ]\n",
      " [0.70754832]] 0.08417137326117684\n",
      "120 [[0.62570019]\n",
      " [0.70757981]] 0.08417136857850827\n",
      "130 [[0.62571463]\n",
      " [0.70759425]] 0.08417136759375353\n",
      "140 [[0.62572125]\n",
      " [0.70760087]] 0.08417136738666177\n",
      "150 [[0.62572429]\n",
      " [0.7076039 ]] 0.08417136734311088\n",
      "160 [[0.62572568]\n",
      " [0.7076053 ]] 0.0841713673339522\n",
      "170 [[0.62572632]\n",
      " [0.70760594]] 0.08417136733202618\n",
      "180 [[0.62572661]\n",
      " [0.70760623]] 0.08417136733162113\n",
      "190 [[0.62572674]\n",
      " [0.70760636]] 0.08417136733153592\n"
     ]
    }
   ],
   "source": [
    "# insert your code\n",
    "# 다른 게이트와 구분하기 위해 W의 이름을 W_or 라 하자.\n",
    "\n",
    "x = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "y = np.array([[0],[1],[1],[1]])\n",
    "W_or = np.random.random((2,1))\n",
    "\n",
    "W_or = grad(W_and, x, y, step = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    y = np.matmul(x, W_or)\n",
    "    if y >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(OR(0,0))\n",
    "print(OR(1,0))\n",
    "print(OR(0,1))\n",
    "print(OR(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAND게이트의 경우 AND게이트를 반전시켜주면 되므로 별도로 학습을 하지 않아도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make nand function\n",
    "\n",
    "def NAND(x1, x2):\n",
    "    return int(not AND(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(NAND(0,0))\n",
    "print(NAND(1,0))\n",
    "print(NAND(0,1))\n",
    "print(NAND(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 XOR게이트를 만들어보자\n",
    "기존과 같은 방식으로 학습시켜보자.\n",
    "\n",
    "* 다음은 XOR게이트의 진리표이다.\n",
    "\n",
    "|x1|x2| y|\n",
    "|-|-|-|\n",
    "|0|0|0|\n",
    "|1|0|1|\n",
    "|0|1|1|\n",
    "|1|1|0|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[0.65385884]\n",
      " [0.139558  ]] 0.3724211965805345\n",
      "10 [[0.65385409]\n",
      " [0.13955324]] 0.3724202929409979\n",
      "20 [[0.65384934]\n",
      " [0.13954849]] 0.3724193894369975\n",
      "30 [[0.65384459]\n",
      " [0.13954374]] 0.372418486068513\n",
      "40 [[0.65383984]\n",
      " [0.13953899]] 0.37241758283552423\n",
      "50 [[0.65383508]\n",
      " [0.13953423]] 0.3724166797380107\n",
      "60 [[0.65383033]\n",
      " [0.13952948]] 0.37241577677595217\n",
      "70 [[0.65382558]\n",
      " [0.13952473]] 0.37241487394932826\n",
      "80 [[0.65382083]\n",
      " [0.13951998]] 0.3724139712581187\n",
      "90 [[0.65381608]\n",
      " [0.13951523]] 0.3724130687023032\n",
      "100 [[0.65381133]\n",
      " [0.13951048]] 0.3724121662818614\n",
      "110 [[0.65380658]\n",
      " [0.13950573]] 0.37241126399677305\n",
      "120 [[0.65380183]\n",
      " [0.13950098]] 0.3724103618470178\n",
      "130 [[0.65379708]\n",
      " [0.13949623]] 0.37240945983257545\n",
      "140 [[0.65379234]\n",
      " [0.13949149]] 0.37240855795342553\n",
      "150 [[0.65378759]\n",
      " [0.13948674]] 0.37240765620954785\n",
      "160 [[0.65378284]\n",
      " [0.13948199]] 0.3724067546009222\n",
      "170 [[0.65377809]\n",
      " [0.13947724]] 0.37240585312752816\n",
      "180 [[0.65377335]\n",
      " [0.1394725 ]] 0.3724049517893455\n",
      "190 [[0.6537686 ]\n",
      " [0.13946775]] 0.37240405058635384\n",
      "200 [[0.65376385]\n",
      " [0.139463  ]] 0.37240314951853304\n",
      "210 [[0.65375911]\n",
      " [0.13945826]] 0.37240224858586285\n",
      "220 [[0.65375436]\n",
      " [0.13945351]] 0.3724013477883229\n",
      "230 [[0.65374962]\n",
      " [0.13944877]] 0.372400447125893\n",
      "240 [[0.65374487]\n",
      " [0.13944402]] 0.3723995465985528\n",
      "250 [[0.65374013]\n",
      " [0.13943928]] 0.3723986462062821\n",
      "260 [[0.65373538]\n",
      " [0.13943453]] 0.3723977459490606\n",
      "270 [[0.65373064]\n",
      " [0.13942979]] 0.37239684582686805\n",
      "280 [[0.6537259 ]\n",
      " [0.13942505]] 0.3723959458396842\n",
      "290 [[0.65372115]\n",
      " [0.1394203 ]] 0.37239504598748885\n",
      "300 [[0.65371641]\n",
      " [0.13941556]] 0.3723941462702617\n",
      "310 [[0.65371167]\n",
      " [0.13941082]] 0.3723932466879824\n",
      "320 [[0.65370693]\n",
      " [0.13940608]] 0.37239234724063097\n",
      "330 [[0.65370218]\n",
      " [0.13940134]] 0.3723914479281869\n",
      "340 [[0.65369744]\n",
      " [0.13939659]] 0.37239054875063016\n",
      "350 [[0.6536927 ]\n",
      " [0.13939185]] 0.37238964970794053\n",
      "360 [[0.65368796]\n",
      " [0.13938711]] 0.3723887508000975\n",
      "370 [[0.65368322]\n",
      " [0.13938237]] 0.37238785202708125\n",
      "380 [[0.65367848]\n",
      " [0.13937763]] 0.37238695338887123\n",
      "390 [[0.65367374]\n",
      " [0.13937289]] 0.3723860548854474\n",
      "400 [[0.653669  ]\n",
      " [0.13936815]] 0.3723851565167895\n",
      "410 [[0.65366426]\n",
      " [0.13936342]] 0.3723842582828772\n",
      "420 [[0.65365953]\n",
      " [0.13935868]] 0.37238336018369045\n",
      "430 [[0.65365479]\n",
      " [0.13935394]] 0.372382462219209\n",
      "440 [[0.65365005]\n",
      " [0.1393492 ]] 0.3723815643894125\n",
      "450 [[0.65364531]\n",
      " [0.13934446]] 0.37238066669428094\n",
      "460 [[0.65364058]\n",
      " [0.13933973]] 0.37237976913379406\n",
      "470 [[0.65363584]\n",
      " [0.13933499]] 0.3723788717079317\n",
      "480 [[0.6536311 ]\n",
      " [0.13933025]] 0.37237797441667364\n",
      "490 [[0.65362637]\n",
      " [0.13932552]] 0.3723770772599996\n",
      "500 [[0.65362163]\n",
      " [0.13932078]] 0.37237618023788943\n",
      "510 [[0.6536169 ]\n",
      " [0.13931605]] 0.3723752833503231\n",
      "520 [[0.65361216]\n",
      " [0.13931131]] 0.3723743865972803\n",
      "530 [[0.65360743]\n",
      " [0.13930658]] 0.37237348997874087\n",
      "540 [[0.65360269]\n",
      " [0.13930184]] 0.37237259349468466\n",
      "550 [[0.65359796]\n",
      " [0.13929711]] 0.37237169714509144\n",
      "560 [[0.65359323]\n",
      " [0.13929238]] 0.37237080092994107\n",
      "570 [[0.65358849]\n",
      " [0.13928764]] 0.3723699048492134\n",
      "580 [[0.65358376]\n",
      " [0.13928291]] 0.37236900890288827\n",
      "590 [[0.65357903]\n",
      " [0.13927818]] 0.3723681130909455\n",
      "600 [[0.6535743 ]\n",
      " [0.13927345]] 0.372367217413365\n",
      "610 [[0.65356956]\n",
      " [0.13926872]] 0.37236632187012647\n",
      "620 [[0.65356483]\n",
      " [0.13926398]] 0.37236542646120996\n",
      "630 [[0.6535601 ]\n",
      " [0.13925925]] 0.3723645311865952\n",
      "640 [[0.65355537]\n",
      " [0.13925452]] 0.372363636046262\n",
      "650 [[0.65355064]\n",
      " [0.13924979]] 0.3723627410401904\n",
      "660 [[0.65354591]\n",
      " [0.13924506]] 0.3723618461683601\n",
      "670 [[0.65354118]\n",
      " [0.13924033]] 0.372360951430751\n",
      "680 [[0.65353645]\n",
      " [0.1392356 ]] 0.37236005682734286\n",
      "690 [[0.65353172]\n",
      " [0.13923088]] 0.3723591623581157\n",
      "700 [[0.653527  ]\n",
      " [0.13922615]] 0.3723582680230495\n",
      "710 [[0.65352227]\n",
      " [0.13922142]] 0.372357373822124\n",
      "720 [[0.65351754]\n",
      " [0.13921669]] 0.37235647975531905\n",
      "730 [[0.65351281]\n",
      " [0.13921196]] 0.3723555858226145\n",
      "740 [[0.65350809]\n",
      " [0.13920724]] 0.3723546920239904\n",
      "750 [[0.65350336]\n",
      " [0.13920251]] 0.37235379835942645\n",
      "760 [[0.65349863]\n",
      " [0.13919778]] 0.37235290482890265\n",
      "770 [[0.65349391]\n",
      " [0.13919306]] 0.37235201143239893\n",
      "780 [[0.65348918]\n",
      " [0.13918833]] 0.3723511181698951\n",
      "790 [[0.65348446]\n",
      " [0.13918361]] 0.37235022504137105\n",
      "800 [[0.65347973]\n",
      " [0.13917888]] 0.3723493320468068\n",
      "810 [[0.65347501]\n",
      " [0.13917416]] 0.3723484391861822\n",
      "820 [[0.65347028]\n",
      " [0.13916943]] 0.37234754645947715\n",
      "830 [[0.65346556]\n",
      " [0.13916471]] 0.37234665386667154\n",
      "840 [[0.65346084]\n",
      " [0.13915999]] 0.3723457614077453\n",
      "850 [[0.65345611]\n",
      " [0.13915526]] 0.3723448690826784\n",
      "860 [[0.65345139]\n",
      " [0.13915054]] 0.3723439768914507\n",
      "870 [[0.65344667]\n",
      " [0.13914582]] 0.3723430848340422\n",
      "880 [[0.65344194]\n",
      " [0.1391411 ]] 0.37234219291043275\n",
      "890 [[0.65343722]\n",
      " [0.13913637]] 0.3723413011206022\n",
      "900 [[0.6534325 ]\n",
      " [0.13913165]] 0.3723404094645307\n",
      "910 [[0.65342778]\n",
      " [0.13912693]] 0.37233951794219805\n",
      "920 [[0.65342306]\n",
      " [0.13912221]] 0.3723386265535843\n",
      "930 [[0.65341834]\n",
      " [0.13911749]] 0.3723377352986692\n",
      "940 [[0.65341362]\n",
      " [0.13911277]] 0.37233684417743285\n",
      "950 [[0.6534089 ]\n",
      " [0.13910805]] 0.37233595318985513\n",
      "960 [[0.65340418]\n",
      " [0.13910333]] 0.3723350623359162\n",
      "970 [[0.65339946]\n",
      " [0.13909861]] 0.3723341716155957\n",
      "980 [[0.65339474]\n",
      " [0.13909389]] 0.37233328102887375\n",
      "990 [[0.65339003]\n",
      " [0.13908918]] 0.37233239057573025\n",
      "1000 [[0.65338531]\n",
      " [0.13908446]] 0.37233150025614525\n",
      "1010 [[0.65338059]\n",
      " [0.13907974]] 0.3723306100700987\n",
      "1020 [[0.65337587]\n",
      " [0.13907502]] 0.3723297200175705\n",
      "1030 [[0.65337116]\n",
      " [0.13907031]] 0.37232883009854056\n",
      "1040 [[0.65336644]\n",
      " [0.13906559]] 0.3723279403129891\n",
      "1050 [[0.65336172]\n",
      " [0.13906087]] 0.372327050660896\n",
      "1060 [[0.65335701]\n",
      " [0.13905616]] 0.37232616114224115\n",
      "1070 [[0.65335229]\n",
      " [0.13905144]] 0.3723252717570046\n",
      "1080 [[0.65334758]\n",
      " [0.13904673]] 0.37232438250516636\n",
      "1090 [[0.65334286]\n",
      " [0.13904201]] 0.37232349338670634\n",
      "1100 [[0.65333815]\n",
      " [0.1390373 ]] 0.37232260440160453\n",
      "1110 [[0.65333344]\n",
      " [0.13903259]] 0.37232171554984106\n",
      "1120 [[0.65332872]\n",
      " [0.13902787]] 0.3723208268313959\n",
      "1130 [[0.65332401]\n",
      " [0.13902316]] 0.3723199382462489\n",
      "1140 [[0.6533193 ]\n",
      " [0.13901845]] 0.37231904979438024\n",
      "1150 [[0.65331458]\n",
      " [0.13901373]] 0.3723181614757699\n",
      "1160 [[0.65330987]\n",
      " [0.13900902]] 0.37231727329039777\n",
      "1170 [[0.65330516]\n",
      " [0.13900431]] 0.372316385238244\n",
      "1180 [[0.65330045]\n",
      " [0.1389996 ]] 0.3723154973192886\n",
      "1190 [[0.65329574]\n",
      " [0.13899489]] 0.3723146095335115\n",
      "1200 [[0.65329103]\n",
      " [0.13899018]] 0.37231372188089273\n",
      "1210 [[0.65328632]\n",
      " [0.13898547]] 0.37231283436141244\n",
      "1220 [[0.65328161]\n",
      " [0.13898076]] 0.37231194697505055\n",
      "1230 [[0.6532769 ]\n",
      " [0.13897605]] 0.37231105972178713\n",
      "1240 [[0.65327219]\n",
      " [0.13897134]] 0.3723101726016023\n",
      "1250 [[0.65326748]\n",
      " [0.13896663]] 0.372309285614476\n",
      "1260 [[0.65326277]\n",
      " [0.13896192]] 0.3723083987603883\n",
      "1270 [[0.65325806]\n",
      " [0.13895721]] 0.37230751203931917\n",
      "1280 [[0.65325335]\n",
      " [0.1389525 ]] 0.37230662545124876\n",
      "1290 [[0.65324865]\n",
      " [0.1389478 ]] 0.37230573899615715\n",
      "1300 [[0.65324394]\n",
      " [0.13894309]] 0.3723048526740242\n",
      "1310 [[0.65323923]\n",
      " [0.13893838]] 0.3723039664848302\n",
      "1320 [[0.65323452]\n",
      " [0.13893368]] 0.3723030804285551\n",
      "1330 [[0.65322982]\n",
      " [0.13892897]] 0.3723021945051789\n",
      "1340 [[0.65322511]\n",
      " [0.13892426]] 0.3723013087146818\n",
      "1350 [[0.65322041]\n",
      " [0.13891956]] 0.3723004230570438\n",
      "1360 [[0.6532157 ]\n",
      " [0.13891485]] 0.37229953753224493\n",
      "1370 [[0.653211  ]\n",
      " [0.13891015]] 0.37229865214026536\n",
      "1380 [[0.65320629]\n",
      " [0.13890544]] 0.3722977668810851\n",
      "1390 [[0.65320159]\n",
      " [0.13890074]] 0.3722968817546843\n",
      "1400 [[0.65319689]\n",
      " [0.13889604]] 0.3722959967610429\n",
      "1410 [[0.65319218]\n",
      " [0.13889133]] 0.37229511190014125\n",
      "1420 [[0.65318748]\n",
      " [0.13888663]] 0.37229422717195915\n",
      "1430 [[0.65318278]\n",
      " [0.13888193]] 0.37229334257647684\n",
      "1440 [[0.65317807]\n",
      " [0.13887722]] 0.3722924581136744\n",
      "1450 [[0.65317337]\n",
      " [0.13887252]] 0.37229157378353184\n",
      "1460 [[0.65316867]\n",
      " [0.13886782]] 0.3722906895860295\n",
      "1470 [[0.65316397]\n",
      " [0.13886312]] 0.37228980552114727\n",
      "1480 [[0.65315927]\n",
      " [0.13885842]] 0.37228892158886534\n",
      "1490 [[0.65315457]\n",
      " [0.13885372]] 0.37228803778916375\n",
      "1500 [[0.65314987]\n",
      " [0.13884902]] 0.37228715412202273\n",
      "1510 [[0.65314517]\n",
      " [0.13884432]] 0.3722862705874223\n",
      "1520 [[0.65314047]\n",
      " [0.13883962]] 0.37228538718534265\n",
      "1530 [[0.65313577]\n",
      " [0.13883492]] 0.37228450391576384\n",
      "1540 [[0.65313107]\n",
      " [0.13883022]] 0.37228362077866606\n",
      "1550 [[0.65312637]\n",
      " [0.13882552]] 0.37228273777402937\n",
      "1560 [[0.65312167]\n",
      " [0.13882083]] 0.372281854901834\n",
      "1570 [[0.65311698]\n",
      " [0.13881613]] 0.37228097216205996\n",
      "1580 [[0.65311228]\n",
      " [0.13881143]] 0.3722800895546875\n",
      "1590 [[0.65310758]\n",
      " [0.13880673]] 0.3722792070796967\n",
      "1600 [[0.65310289]\n",
      " [0.13880204]] 0.3722783247370677\n",
      "1610 [[0.65309819]\n",
      " [0.13879734]] 0.37227744252678063\n",
      "1620 [[0.65309349]\n",
      " [0.13879264]] 0.37227656044881585\n",
      "1630 [[0.6530888 ]\n",
      " [0.13878795]] 0.37227567850315324\n",
      "1640 [[0.6530841 ]\n",
      " [0.13878325]] 0.37227479668977304\n",
      "1650 [[0.65307941]\n",
      " [0.13877856]] 0.3722739150086555\n",
      "1660 [[0.65307471]\n",
      " [0.13877386]] 0.3722730334597806\n",
      "1670 [[0.65307002]\n",
      " [0.13876917]] 0.3722721520431286\n",
      "1680 [[0.65306533]\n",
      " [0.13876448]] 0.3722712707586797\n",
      "1690 [[0.65306063]\n",
      " [0.13875978]] 0.3722703896064141\n",
      "1700 [[0.65305594]\n",
      " [0.13875509]] 0.37226950858631186\n",
      "1710 [[0.65305125]\n",
      " [0.1387504 ]] 0.3722686276983533\n",
      "1720 [[0.65304655]\n",
      " [0.1387457 ]] 0.3722677469425184\n",
      "1730 [[0.65304186]\n",
      " [0.13874101]] 0.3722668663187876\n",
      "1740 [[0.65303717]\n",
      " [0.13873632]] 0.3722659858271408\n",
      "1750 [[0.65303248]\n",
      " [0.13873163]] 0.3722651054675584\n",
      "1760 [[0.65302779]\n",
      " [0.13872694]] 0.37226422524002045\n",
      "1770 [[0.6530231 ]\n",
      " [0.13872225]] 0.3722633451445073\n",
      "1780 [[0.65301841]\n",
      " [0.13871756]] 0.37226246518099915\n",
      "1790 [[0.65301372]\n",
      " [0.13871287]] 0.37226158534947607\n",
      "1800 [[0.65300903]\n",
      " [0.13870818]] 0.3722607056499183\n",
      "1810 [[0.65300434]\n",
      " [0.13870349]] 0.3722598260823061\n",
      "1820 [[0.65299965]\n",
      " [0.1386988 ]] 0.3722589466466195\n",
      "1830 [[0.65299496]\n",
      " [0.13869411]] 0.37225806734283906\n",
      "1840 [[0.65299027]\n",
      " [0.13868942]] 0.3722571881709446\n",
      "1850 [[0.65298558]\n",
      " [0.13868474]] 0.3722563091309167\n",
      "1860 [[0.6529809 ]\n",
      " [0.13868005]] 0.37225543022273533\n",
      "1870 [[0.65297621]\n",
      " [0.13867536]] 0.37225455144638075\n",
      "1880 [[0.65297152]\n",
      " [0.13867067]] 0.3722536728018333\n",
      "1890 [[0.65296684]\n",
      " [0.13866599]] 0.3722527942890731\n",
      "1900 [[0.65296215]\n",
      " [0.1386613 ]] 0.37225191590808043\n",
      "1910 [[0.65295746]\n",
      " [0.13865662]] 0.3722510376588355\n",
      "1920 [[0.65295278]\n",
      " [0.13865193]] 0.37225015954131857\n",
      "1930 [[0.65294809]\n",
      " [0.13864725]] 0.37224928155550996\n",
      "1940 [[0.65294341]\n",
      " [0.13864256]] 0.3722484037013898\n",
      "1950 [[0.65293873]\n",
      " [0.13863788]] 0.3722475259789384\n",
      "1960 [[0.65293404]\n",
      " [0.13863319]] 0.37224664838813604\n",
      "1970 [[0.65292936]\n",
      " [0.13862851]] 0.3722457709289628\n",
      "1980 [[0.65292467]\n",
      " [0.13862383]] 0.372244893601399\n",
      "1990 [[0.65291999]\n",
      " [0.13861914]] 0.37224401640542504\n"
     ]
    }
   ],
   "source": [
    "# insert your code\n",
    "# 다른 게이트와 구분하기 위해 W의 이름을 W_xor 라 하자.\n",
    "\n",
    "x = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "W_xor = np.random.random((2,1))\n",
    "\n",
    "W_xor = grad(W_xor, x, y, learning_rate = 1e-5, step = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR(x1, x2):\n",
    "    x = np.array([x1, x2])\n",
    "    y = np.matmul(x, W_xor)\n",
    "    if y >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(XOR(0,0))\n",
    "print(XOR(1,0))\n",
    "print(XOR(0,1))\n",
    "print(XOR(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아마 아무리 학습을 하여도 제대로 된 결과가 나오지 않을 것 이다. \n",
    "이는 단순 직선으로는 XOR게이트를 만들 수 없기 떄문이다.\n",
    "\n",
    "<img src=\"./images/2.png\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 비선형 구조는 선형 구조의 분류자를 층을 쌓아서 조합해 비선형 구조로 만들어야한다.\n",
    "\n",
    "따라서 우리는 우리가 만든 AND, OR, NAND게이트를 조합하여 XOR를 만들어야 한다.\n",
    "\n",
    "\n",
    "\n",
    "##### hint: 조합 방법은 다음과 같다.\n",
    "\n",
    "<img src=\"./images/3.png\" width=60%/>\n",
    "\n",
    "\n",
    "위 방법은 마치 신경망 구조와 같다. 따라서 이를 다시 신경망 처럼 표현하면 다음과 같다.\n",
    "\n",
    "<img src=\"./images/4.png\" width=80%/>\n",
    "\n",
    "위 구조를 이용해서 XOR게이트 함수를 직접 구현해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make XOR gate\n",
    "\n",
    "def XOR(x1, x2):\n",
    "    s1 = NAND(x1,x2)\n",
    "    s2 = OR(x1,x2)\n",
    "    return AND(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(XOR(0,0))\n",
    "print(XOR(1,0))\n",
    "print(XOR(0,1))\n",
    "print(XOR(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example1\n",
    "\n",
    "### tensorflow 기초 실습\n",
    "\n",
    "앞에서 XOR게이트를 만들 때 본 신경망 구조는 tensorflow의 작동방식과 일치한다.\n",
    "\n",
    "tensorflow를 익히기 위해 간단한 예제를 실습해 보자.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./images/5.png\" width=70%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define a computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.placeholder(tf.int32, name=\"input_a\")\n",
    "b = tf.placeholder(tf.int32, name=\"input_b\")\n",
    "\n",
    "c = tf.add(a, b, name=\"add\")\n",
    "d = tf.multiply(a, b, name=\"multiply\")\n",
    "e = tf.subtract(c, d, name=\"subtract\")\n",
    "out = tf.add(b, e, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 tensorflow를 이용하여 operation(Node)와 tensor(edge)를 정의하여 그래프를 정의해준다. \n",
    "\n",
    "이는 단순히 그래프를 정의한 것으로 Session 객체로 run 하기 전까지는 실행이 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: -8\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "input_data = { a: 7, b: 3 }\n",
    "result = sess.run(out, feed_dict=input_data)\n",
    "print(\"out:\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "### tensorflow를 이용하여 계산해보기\n",
    "\n",
    "다음 그래프를 정의하고 계산해 보자.\n",
    "<img src=\"./images/6.png\" width=70%/>\n",
    "\n",
    "\n",
    "tensorflow의 수학 관련 Operations 에 관한 설명은 [이곳을 참고하자](https://www.tensorflow.org/api_guides/python/math_ops)\n",
    "\n",
    "그래프 실행 후 결과는 다음과 같다\n",
    "\n",
    "|In | Out|\n",
    "|---|----|\n",
    "|1, 2, 3| 15|\n",
    "|-1, -2, 3| -3|\n",
    "|123, 456, 789|44613795|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "in1 = tf.placeholder(tf.int32, name=\"input_1\")\n",
    "in2 = tf.placeholder(tf.int32, name=\"input_2\")\n",
    "in3 = tf.placeholder(tf.int32, name=\"input_3\")\n",
    "\n",
    "prod_list = [in1, in2, in3]\n",
    "a = tf.add(in1, in2, name = \"add\")\n",
    "b = tf.reduce_prod(prod_list, name = \"product\")\n",
    "c = tf.multiply(in2, in3, name = \"mul\")\n",
    "\n",
    "sum_list = [a, b, c]\n",
    "out = tf.reduce_sum(sum_list, name='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: 15\n",
      "out: -3\n",
      "out: 44613795\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "input_data1 = { in1: 1, in2: 2, in3: 3}\n",
    "input_data2 = { in1: -1, in2: -2, in3: 3}\n",
    "input_data3 = { in1: 123, in2: 456, in3: 789}\n",
    "\n",
    "\n",
    "result1 = sess.run(out, feed_dict=input_data1)\n",
    "result2 = sess.run(out, feed_dict=input_data2)\n",
    "result3 = sess.run(out, feed_dict=input_data3)\n",
    "print(\"out:\",result1)\n",
    "print(\"out:\",result2)\n",
    "print(\"out:\",result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
